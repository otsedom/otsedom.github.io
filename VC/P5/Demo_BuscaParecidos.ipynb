{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\otsed\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import FaceNormalizationUtils as faceutils\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "# mode and so on\n",
    "from collections import Counter\n",
    "\n",
    "#MTCNN face detector\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "#deepface\n",
    "from deepface import DeepFace\n",
    "#from deepface.commons import functions\n",
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embs(X, batch_size=2):\n",
    "    norm_images = prewhiten(X)\n",
    "    pd = []\n",
    "    for start in range(0, len(norm_images), batch_size):\n",
    "        pd.append(model.predict_on_batch(norm_images[start:start+batch_size]))\n",
    "        \n",
    "    return l2_normalize(np.concatenate(pd))\n",
    "\n",
    "def calc_embs_v2(X):\n",
    "    norm_images = prewhiten(X)\n",
    "    pd = []\n",
    "    for start in range(0, len(norm_images)):\n",
    "        embedding_objs = DeepFace.represent(img_path = norm_images[start],model_name  = \"Facenet\", enforce_detection = False) \n",
    "        pd.append(embedding_objs[0][\"embedding\"] )\n",
    "    return l2_normalize(np.concatenate(pd))\n",
    "\n",
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n",
    "    return output\n",
    "\n",
    "def prewhiten(x):\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y\n",
    "\n",
    "# Designed for UTKFace as filename contains biometric information\n",
    "def TrainEmbedingsUTKFace(folders,outputfile):\n",
    "    nimgs = 0\n",
    "    Xorig = []\n",
    "    X = []\n",
    "    Gender = []\n",
    "    Age = []\n",
    "    Etnia = []\n",
    "    \n",
    "    for directory in folders:\n",
    "        #print(directory)\n",
    "        for path, subdirs, files in os.walk(directory):\n",
    "            for name in files:\n",
    "                #print(name)\n",
    "                if name.endswith(\".jpg\") and nimgs < 2000:\n",
    "                #if name.endswith(\".png\"): # and nimgs < 3000:\n",
    "                    img_path = os.path.join(path, name)\n",
    "                    #print(img_path)\n",
    "\n",
    "                    image = cv2.imread(img_path)\n",
    "\n",
    "                    if (type(image) is np.ndarray):\n",
    "                        #print(img_path)\n",
    "                        # Search face \n",
    "                        values = DetectLargestFaceEyesMTCNN(image)\n",
    "                        if values is not None:\n",
    "                            #print(nimgs)\n",
    "                            face, eyes, shape = values\n",
    "\n",
    "                            # draws container\n",
    "                            [x, y, w, h] = face\n",
    "                            if x > -1:\n",
    "                                # Eyes\n",
    "                                [lex, ley, rex, rey] = eyes\n",
    "                                if lex > -1:\n",
    "                                    \n",
    "                                    B, G, R = cv2.split(image)\n",
    "\n",
    "                                    # Normalize for Facenet\n",
    "                                    normalizatorHS.normalize_gray_img(B, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Bnorm = normalizatorHS.normf_image\n",
    "                                    normalizatorHS.normalize_gray_img(G, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Gnorm = normalizatorHS.normf_image\n",
    "                                    normalizatorHS.normalize_gray_img(R, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Rnorm = normalizatorHS.normf_image\n",
    "                                    NormBGRHS = cv2.merge((Bnorm, Gnorm, Rnorm))\n",
    "                                    #cv2.imshow(\"Normalized\", NormBGRHS)\n",
    "\n",
    "                                    # Cropping from HS for facenet\n",
    "                                    # Usa nyoki https://github.com/nyoki-mtl/keras-facenet\n",
    "                                    NormBGR = NormBGRHS[35:115, 39:119, :]\n",
    "\n",
    "                                    # SOft biometric data are extracted from filename\n",
    "                                    if NormBGR is not None:\n",
    "                                        nimgs = nimgs + 1\n",
    "                                        #print(nimgs)\n",
    "\n",
    "                                        fsub = name.find('_')\n",
    "                                        sage = name[:fsub]\n",
    "                                        #print(sage)\n",
    "                                        sub = name[fsub + 1:]\n",
    "                                        fsub = sub.find('_')\n",
    "                                        sgender = sub[:fsub]\n",
    "                                        sub = sub[fsub + 1:]\n",
    "                                        fsub = sub.find('_')\n",
    "                                        setnia = sub[:fsub]\n",
    "\n",
    "                                        # print(name)\n",
    "                                        # print(sage)\n",
    "                                        # print(sgender)\n",
    "                                        # print(setnia)\n",
    "                                        # print(nimgs)\n",
    "\n",
    "                                        # Facenet kearas expects 160x160\n",
    "                                        #imaged = cv2.resize(NormBGR, (160, 160))\n",
    "                                        \n",
    "                                        # Obtiene embeddings\n",
    "                                        imaged = cv2.resize(NormBGR, dim, interpolation = cv2.INTER_AREA)\n",
    "                                        \n",
    "                                        # Mantengo originales para mostrar parecido\n",
    "                                        imager = cv2.resize(image, (200, 200), interpolation=cv2.INTER_AREA)\n",
    "                                        Xorig.append(imager)\n",
    "                                        # Facenet\n",
    "                                        X.append(imaged)\n",
    "                                        Gender.append(sgender)\n",
    "                                        Age.append(sage)\n",
    "                                        Etnia.append(setnia)\n",
    "    \n",
    "    if nimgs > 0:\n",
    "        # Compute embeddings \n",
    "        embs = calc_embs_v2(np.array(X))\n",
    "\n",
    "        print(\"Salvando embs\")\n",
    "        fid = open(outputfile, \"wb\")\n",
    "\n",
    "        pickle.dump([nimgs, X, embs, Age, Gender, Etnia], fid)\n",
    "        print(embs.shape)\n",
    "        fid.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSimilarNN(X,idxsimilar, nbrs, embs, Title):\n",
    "    print(embs)\n",
    "    # Gets nearest neighbors\n",
    "    distance, indices = nbrs.kneighbors(embs)\n",
    "    minp = indices[0][0]\n",
    "\n",
    "    # Keeps copy of last frames NN\n",
    "    idxsimilar.extend(indices[0])\n",
    "\n",
    "    # Remove old ones when more than nframes accumulated\n",
    "    if len(idxsimilar) > kvecinos * nframeskvecinos:\n",
    "        idxsimilar = idxsimilar[kvecinos:]\n",
    "\n",
    "    # print('Lista con ' + str(len(idxsimilar)) + ' ' + str(idxsimilar) + '\\n')\n",
    "\n",
    "    # enough history\n",
    "    if len(idxsimilar) > kvecinos:\n",
    "        minp = GetClosesetMode(idxsimilar)\n",
    "\n",
    "    # Larger for visualization\n",
    "    imageS = cv2.resize(X[minp], (320, 320))\n",
    "\n",
    "    # Soft biometrics labels\n",
    "    if len(idxsimilar) > 0:\n",
    "        modesG = stats.mode([Gender[int(i)] for i in idxsimilar])\n",
    "        modesE = stats.mode([Etnia[int(i)] for i in idxsimilar])\n",
    "        edad = np.array([int(Age[int(i)]) for i in idxsimilar]).mean()\n",
    "\n",
    "        if modesG[0][0] == '0':\n",
    "            gen = 'M'\n",
    "        else:\n",
    "            gen = 'F'\n",
    "\n",
    "        if modesE[0][0] == '0':\n",
    "            et = 'B'\n",
    "        elif modesE[0][0] == '1':\n",
    "            et = 'N'\n",
    "        elif modesE[0][0] == '2':\n",
    "            et = 'A'\n",
    "        elif modesE[0][0] == '3':\n",
    "            et = 'H'\n",
    "        else:\n",
    "            et = 'L'\n",
    "\n",
    "        cv2.putText(imageS, '%s %s %d (%s)' % (gen, et, int(edad), Age[minp]), (10, 30), font, 0.5, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(Title, imageS)\n",
    "\n",
    "    return idxsimilar\n",
    "\n",
    "def getLargestMTCNNBB(objects):\n",
    "        if len(objects) < 1:\n",
    "            return -1\n",
    "        elif len(objects) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            areas = [ (det['box'][2]*det['box'][3]) for det in objects ]\n",
    "            return np.argmax(areas)\n",
    "        \n",
    "def DetectLargestFaceEyesMTCNN(img):\n",
    "    global detectormtcnn\n",
    "    \n",
    "    results = detectormtcnn.detect_faces(img)\n",
    "\n",
    "    if not results is None:\n",
    "        index = getLargestMTCNNBB(results)\n",
    "\n",
    "        if len(results) < 1:\n",
    "            return None\n",
    "\n",
    "        # laergest face\n",
    "        face_info = results[index]\n",
    "\n",
    "        #print(face_info)\n",
    "\n",
    "        [x, y, w, h] = face_info['box']\n",
    "        le = face_info['keypoints']['left_eye']\n",
    "        re = face_info['keypoints']['right_eye']\n",
    "\n",
    "        return [x,y,w,h], [le[0], le[1], re[0], re[1]], [face_info['keypoints']['left_eye'], face_info['keypoints']['right_eye'],\n",
    "                      face_info['keypoints']['nose'], face_info['keypoints']['mouth_left'],\n",
    "                      face_info['keypoints']['mouth_right']]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def ResetDetectionCounters():\n",
    "    global nconsecutivenodetected, nconsecutivedetected, idxsimilarFN\n",
    "\n",
    "    nconsecutivenodetected = nconsecutivenodetected + 1\n",
    "    if nconsecutivenodetected > 10:\n",
    "        nconsecutivedetected = 0\n",
    "        idxsimilarFN = []\n",
    "        \n",
    "def GetClosesetMode(list):\n",
    "    # Get occurrences list\n",
    "    occ = Counter(list)\n",
    "\n",
    "    # Get the mode, if multiple modes present, gets the clostest observing position among beigbors\n",
    "    prima = 0\n",
    "    maxpun = 0\n",
    "    for neighbor, count in occ.most_common(10):\n",
    "        if prima == 0:\n",
    "            prima = 1\n",
    "            mode = count\n",
    "        else:\n",
    "            if count < mode:\n",
    "                break\n",
    "\n",
    "        #print('%s: %7d' % (neighbor, count))\n",
    "\n",
    "        # using enumerate()\n",
    "        # to find indices for 3\n",
    "        neighbor_pos = [i for i, value in enumerate(list) if value == neighbor]\n",
    "        # printing resultant list\n",
    "        #print(\"New indices list : \" + str(neighbor_pos))\n",
    "\n",
    "        pun = 0\n",
    "        for pos in neighbor_pos:\n",
    "            pun = pun + kvecinos - (pos % kvecinos)\n",
    "        #print(pun)\n",
    "        if pun > maxpun:\n",
    "            maxpun = pun\n",
    "            closest_neighbor = neighbor\n",
    "\n",
    "    return closest_neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model\n",
      "2000\n",
      "[ 0.19310383  0.0031643   0.12478592 -0.20989816  0.03106969  0.1003861\n",
      "  0.03888208  0.10858168  0.04909337  0.06012051  0.01520464  0.01530008\n",
      "  0.03777692 -0.04863881 -0.01468688 -0.11398428  0.06734373  0.08293412\n",
      "  0.01901399  0.0459023   0.12020404 -0.11268966 -0.01748156  0.04482151\n",
      "  0.04064983 -0.1285081  -0.06753954  0.10069403 -0.1781441  -0.05199728\n",
      " -0.02251568  0.13836882 -0.13299906  0.06738377 -0.07669103 -0.12358224\n",
      "  0.03400111 -0.00520453 -0.00024235  0.09055389  0.03688106 -0.07935634\n",
      "  0.13672787 -0.03770848 -0.0936017  -0.02356653  0.05163802 -0.08694541\n",
      "  0.04869903 -0.08455293 -0.15029472  0.05402508  0.05865078 -0.13785231\n",
      "  0.00127104  0.00553483  0.03424353 -0.0839912   0.00178772  0.09450344\n",
      "  0.03688648  0.15182371 -0.02889229  0.18463968  0.13324045 -0.05565208\n",
      "  0.11443888  0.01673327  0.11453298 -0.02560252 -0.02619596  0.06534784\n",
      "  0.04913598  0.00683489  0.08538314  0.01496218 -0.08107303 -0.16824494\n",
      " -0.08475927 -0.00872247  0.1258791   0.00637132  0.03079398 -0.12283961\n",
      "  0.10889985 -0.00107293 -0.07826985 -0.07647106  0.02697838  0.19036786\n",
      " -0.09573037 -0.05369278 -0.16791728  0.00377347  0.12528895  0.01943189\n",
      " -0.07722586 -0.03521525  0.10621862  0.06572608  0.12794003 -0.0003537\n",
      " -0.10092794 -0.17846818 -0.17992738  0.08717532 -0.04837705 -0.07301348\n",
      "  0.03459212 -0.1266738  -0.07351887  0.14002557 -0.08043242 -0.07271586\n",
      "  0.04233726  0.01077241 -0.07225476  0.04182797  0.04810226  0.09191547\n",
      "  0.08976297  0.00514785 -0.02072191 -0.03547558 -0.11052185 -0.03988084\n",
      "  0.07636774 -0.06926827]\n",
      "Camera 2\n",
      "2000\n",
      "(128,)\n",
      "[-0.04256672 -0.11473004 -0.02355625 -0.12135403  0.09868719  0.03064655\n",
      " -0.01051323 -0.05323723 -0.00813854  0.05281385  0.06169069 -0.08097452\n",
      "  0.0390095  -0.08307895 -0.0248801   0.11976844  0.0880751  -0.08933508\n",
      "  0.07114759 -0.14221126  0.04878055  0.04171614 -0.04691983 -0.00562779\n",
      "  0.06325153 -0.0256941   0.13162512  0.11859584 -0.03221897 -0.00493992\n",
      " -0.06041448  0.10458144  0.00667513  0.01054465  0.13789662 -0.12098879\n",
      "  0.04699454  0.03169855  0.12985136  0.02826554  0.06495967 -0.00139198\n",
      " -0.02722496 -0.04632006 -0.11827609 -0.1461756   0.0060706  -0.0544685\n",
      "  0.14308167  0.07179418 -0.13272775  0.08077845  0.057591   -0.11462518\n",
      "  0.03224185  0.03295977  0.14364871  0.01466678 -0.08844637 -0.24410139\n",
      " -0.11754058 -0.0944346  -0.01675603  0.19309601 -0.08529556  0.13027069\n",
      "  0.08135856  0.01502358  0.07589525 -0.12541221 -0.02316528 -0.11529614\n",
      "  0.05349674 -0.11267369 -0.01267495 -0.0273954   0.00678179  0.09056694\n",
      " -0.06047326 -0.08010712 -0.15997348 -0.05002134  0.04559634  0.07208384\n",
      "  0.11303577  0.08709139  0.11507318 -0.05221154 -0.16770481  0.23177046\n",
      "  0.1072428   0.01451071 -0.00664449  0.01068052  0.21050184  0.05344261\n",
      " -0.00827356 -0.07132468 -0.20371191 -0.02740514 -0.06053008  0.15654729\n",
      "  0.04856967 -0.08960078  0.00276733  0.03962938 -0.04356544  0.01706871\n",
      " -0.09241164 -0.11112215  0.05847767  0.0538338   0.08082592  0.11658586\n",
      "  0.00276324  0.03153465  0.04604751  0.09102811  0.05252675 -0.01445156\n",
      " -0.04285631  0.01903454  0.08048455  0.01111568  0.07699907 -0.13632336\n",
      "  0.04992889  0.04852034]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-0.04256672 -0.11473004 -0.02355625 -0.12135403  0.09868719  0.03064655\n -0.01051323 -0.05323723 -0.00813854  0.05281385  0.06169069 -0.08097452\n  0.0390095  -0.08307895 -0.0248801   0.11976844  0.0880751  -0.08933508\n  0.07114759 -0.14221126  0.04878055  0.04171614 -0.04691983 -0.00562779\n  0.06325153 -0.0256941   0.13162512  0.11859584 -0.03221897 -0.00493992\n -0.06041448  0.10458144  0.00667513  0.01054465  0.13789662 -0.12098879\n  0.04699454  0.03169855  0.12985136  0.02826554  0.06495967 -0.00139198\n -0.02722496 -0.04632006 -0.11827609 -0.1461756   0.0060706  -0.0544685\n  0.14308167  0.07179418 -0.13272775  0.08077845  0.057591   -0.11462518\n  0.03224185  0.03295977  0.14364871  0.01466678 -0.08844637 -0.24410139\n -0.11754058 -0.0944346  -0.01675603  0.19309601 -0.08529556  0.13027069\n  0.08135856  0.01502358  0.07589525 -0.12541221 -0.02316528 -0.11529614\n  0.05349674 -0.11267369 -0.01267495 -0.0273954   0.00678179  0.09056694\n -0.06047326 -0.08010712 -0.15997348 -0.05002134  0.04559634  0.07208384\n  0.11303577  0.08709139  0.11507318 -0.05221154 -0.16770481  0.23177046\n  0.1072428   0.01451071 -0.00664449  0.01068052  0.21050184  0.05344261\n -0.00827356 -0.07132468 -0.20371191 -0.02740514 -0.06053008  0.15654729\n  0.04856967 -0.08960078  0.00276733  0.03962938 -0.04356544  0.01706871\n -0.09241164 -0.11112215  0.05847767  0.0538338   0.08082592  0.11658586\n  0.00276324  0.03153465  0.04604751  0.09102811  0.05252675 -0.01445156\n -0.04285631  0.01903454  0.08048455  0.01111568  0.07699907 -0.13632336\n  0.04992889  0.04852034].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_FN))\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(embs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 118\u001b[0m     idxsimilarFN \u001b[38;5;241m=\u001b[39m \u001b[43mGetSimilarNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_FN\u001b[49m\u001b[43m,\u001b[49m\u001b[43midxsimilarFN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbrsFN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFN Me recuerdas a ...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     ResetDetectionCounters()\n",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m, in \u001b[0;36mGetSimilarNN\u001b[1;34m(X, idxsimilar, nbrs, embs, Title)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(embs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Gets nearest neighbors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m distance, indices \u001b[38;5;241m=\u001b[39m \u001b[43mnbrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m minp \u001b[38;5;241m=\u001b[39m indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Keeps copy of last frames NN\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\otsed\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:825\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    823\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mc:\\Users\\otsed\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\otsed\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\sklearn\\utils\\validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1049\u001b[0m             )\n\u001b[1;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.04256672 -0.11473004 -0.02355625 -0.12135403  0.09868719  0.03064655\n -0.01051323 -0.05323723 -0.00813854  0.05281385  0.06169069 -0.08097452\n  0.0390095  -0.08307895 -0.0248801   0.11976844  0.0880751  -0.08933508\n  0.07114759 -0.14221126  0.04878055  0.04171614 -0.04691983 -0.00562779\n  0.06325153 -0.0256941   0.13162512  0.11859584 -0.03221897 -0.00493992\n -0.06041448  0.10458144  0.00667513  0.01054465  0.13789662 -0.12098879\n  0.04699454  0.03169855  0.12985136  0.02826554  0.06495967 -0.00139198\n -0.02722496 -0.04632006 -0.11827609 -0.1461756   0.0060706  -0.0544685\n  0.14308167  0.07179418 -0.13272775  0.08077845  0.057591   -0.11462518\n  0.03224185  0.03295977  0.14364871  0.01466678 -0.08844637 -0.24410139\n -0.11754058 -0.0944346  -0.01675603  0.19309601 -0.08529556  0.13027069\n  0.08135856  0.01502358  0.07589525 -0.12541221 -0.02316528 -0.11529614\n  0.05349674 -0.11267369 -0.01267495 -0.0273954   0.00678179  0.09056694\n -0.06047326 -0.08010712 -0.15997348 -0.05002134  0.04559634  0.07208384\n  0.11303577  0.08709139  0.11507318 -0.05221154 -0.16770481  0.23177046\n  0.1072428   0.01451071 -0.00664449  0.01068052  0.21050184  0.05344261\n -0.00827356 -0.07132468 -0.20371191 -0.02740514 -0.06053008  0.15654729\n  0.04856967 -0.08960078  0.00276733  0.03962938 -0.04356544  0.01706871\n -0.09241164 -0.11112215  0.05847767  0.0538338   0.08082592  0.11658586\n  0.00276324  0.03153465  0.04604751  0.09102811  0.05252675 -0.01445156\n -0.04285631  0.01903454  0.08048455  0.01111568  0.07699907 -0.13632336\n  0.04992889  0.04852034].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Face detector\n",
    "detectormtcnn = MTCNN()\n",
    "# Normalization utilities\n",
    "normalizatorHS = faceutils.Normalization()\n",
    "\n",
    "# Embeddings deepface\n",
    "model = DeepFace.build_model(\"Facenet\")\n",
    "dim = model.input_shape\n",
    "\n",
    "# 1 to create a new appearance dataset\n",
    "creadataset = 0\n",
    "if creadataset == 1:\n",
    "    print(\"Create model\")\n",
    "    #Dataset folders\n",
    "    folders = [\"D:/Datasets/Caras/UTKFace/part1\", \"D:/Datasets/Caras/UTKFace/part2\",\n",
    "               \"D:/Datasets/Caras/UTKFace/part3\"]\n",
    "    TrainEmbedingsUTKFace(folders,\"D:/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\")\n",
    "\n",
    "#Load appearance dataset\n",
    "print(\"Reading model\")\n",
    "#MODIFICAR EN BASE A TU RUTA\n",
    "#fid = open(\"C:/Users/otsed/Desktop/Docencia/VC/FACEScode_models/UTKFace_DLIB_Nyoki.obj\", \"rb\") \n",
    "fid = open(\"C:/Users/otsed/Desktop/Docencia/VC/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\", \"rb\") #Modelo reducido\n",
    "#fid = open(\"E:/FACEScode_models/UTKFace_DLIB_Nyoki.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 20k\n",
    "#fid = open(\"D:/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 2k\n",
    "nimgs, X_FN, embsFN, Age, Gender, Etnia = pickle.load(fid)\n",
    "\n",
    "print(len(embsFN))\n",
    "print(embsFN[0])\n",
    "\n",
    "# Tree for KNN search\n",
    "kvecinos = 10\n",
    "nframeskvecinos = 5\n",
    "nbrsFN = NearestNeighbors(n_neighbors=kvecinos).fit(embsFN)\n",
    "\n",
    "# Fonts\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Webcam connection, check unitl one is located\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Check for other cameras\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(2)\n",
    "        if not cap.isOpened():\n",
    "            print('Camera error')\n",
    "            exit(0)\n",
    "        else:\n",
    "            print('Camera 2')\n",
    "    else:\n",
    "        print('Camera 1')\n",
    "else:\n",
    "    print('Camera 2')\n",
    "    \n",
    "#Set camera resolution\n",
    "cap.set(3,640);\n",
    "cap.set(4,480);\n",
    "\n",
    "# Initializations\n",
    "debug = 0\n",
    "nconsecutivedetected = 0\n",
    "nconsecutivenodetected = 0\n",
    "idxsimilarFN = []\n",
    "\n",
    "while True:\n",
    "    # Get frame\n",
    "    t = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    # For HS normalization split channels\n",
    "    B, G, R = cv2.split(frame)\n",
    "\n",
    "    # Search face \n",
    "    values = DetectLargestFaceEyesMTCNN(frame)\n",
    "    if values is not None:\n",
    "        face, eyes, shape = values\n",
    "\n",
    "        #draws face container\n",
    "        [x, y , w, h] = face\n",
    "        if x > -1:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "            # draws eyes and mask if available\n",
    "            [lex, ley, rex, rey] = eyes\n",
    "            if lex > -1:                \n",
    "                nconsecutivedetected = nconsecutivedetected + 1  #11?\n",
    "                nconsecutivenodetected = 0\n",
    "                    \n",
    "                # Show detected facial elements\n",
    "                for (x, y) in shape:\n",
    "                    cv2.circle(frame, (x, y), 2, (255, 255, 255), -1)\n",
    "                cv2.circle(frame, ((int)(lex), (int)(ley)), 4, (0, 0, 255), -1)\n",
    "                cv2.circle(frame, ((int)(rex), (int)(rey)), 4, (0, 255, 0), -1)\n",
    "\n",
    "                # HS normalization for facenet\n",
    "                normalizatorHS.normalize_gray_img(B, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Bnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(G, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Gnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(R, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Rnorm = normalizatorHS.normf_image\n",
    "                NormBGRHS = cv2.merge((Bnorm, Gnorm, Rnorm))\n",
    "                cv2.imshow(\"Normalized\", NormBGRHS)\n",
    "                    \n",
    "                # Cropping from HS for facenet\n",
    "                # De HS a lo proximadamente usa nyok https://github.com/nyoki-mtl/keras-facenet\n",
    "                NormBGR = NormBGRHS[35:115, 39:119, :]\n",
    "                cv2.imshow(\"Normalizedc\", NormBGR)\n",
    "                 \n",
    "                # Obtiene embeddings\n",
    "                img1 = cv2.resize(NormBGR, dim, interpolation = cv2.INTER_AREA)\n",
    "                #embs = model.predict(img1[None,...])\n",
    "                embs = calc_embs_v2(np.array([img1]))\n",
    "                \n",
    "\n",
    "                print(len(X_FN))\n",
    "                print(embs.shape)\n",
    "                idxsimilarFN = GetSimilarNN(X_FN,idxsimilarFN, nbrsFN, embs.reshape(1, -1), 'FN Me recuerdas a ...')\n",
    "                \n",
    "            else:\n",
    "                ResetDetectionCounters()\n",
    "        else:\n",
    "            ResetDetectionCounters()\n",
    "    else:\n",
    "        ResetDetectionCounters()\n",
    "\n",
    "    if debug:\n",
    "        print(\"Processing time : {:.3f}\".format(time.time() - t))\n",
    "\n",
    "    # Show resulting image\n",
    "    cv2.imshow('Cam', frame)\n",
    "    \n",
    "    # Esc to finish\n",
    "    tec = cv2.waitKey(5)\n",
    "    if tec & tec == 27:  # Esc\n",
    "        break  \n",
    "\n",
    "# Close windows and release camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
