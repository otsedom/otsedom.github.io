{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "\n",
    "# Local descriptors\n",
    "from skimage.feature import hog\n",
    "from skimage import  exposure\n",
    "from skimage import feature\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de funciones\n",
    "\n",
    "\n",
    "lbphist Definido un número de celdas, calcula el histograma LBP de cada una, concatenando los histogramas resultantes\n",
    "\n",
    "GetHOG y GetLPB obtención de descriptores locales\n",
    "\n",
    "GetPredictions y GetPredictionsandProbs obtienen las etiquetas o probabilidades para un conjunto de test utilizando clasificadores SVM\n",
    "\n",
    "LoadDataset Carga conjunto de datos. Se proporciona la carpeta, a través de la variable folder, donde cada subcarpeta se corresponde con una clase. Cada clase contiene muestras en forma de imágenes jpg, todas del mismo tamaño \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbphist(gray, ncellsx, ncellsy, width, height, type):\n",
    "    #Rejilla de histogramas\n",
    "    pxpercellx = int(width/ncellsx)\n",
    "    pxpercelly = int(height/ncellsy)\n",
    "\n",
    "    # Rejillas de igual tamaño y centradas en la imagen de entrada\n",
    "    ofx = int((width - int(pxpercellx)*ncellsx)/2)\n",
    "    ofy = int((height - int(pxpercelly)*ncellsy)/2)\n",
    "   \n",
    "    #Obtiene histograma LBP para cada celda\n",
    "    LBPu_hist = []\n",
    "    for i in range(0,ncellsy):\n",
    "        for j in range(0,ncellsx):\n",
    "            # Extrae celda\n",
    "            roi=gray[ofy+i*pxpercelly:ofy+(i+1)*pxpercelly,ofx+j*pxpercellx:ofx+(j+1)*pxpercellx]\n",
    "            # Obtiene imagen LBP\n",
    "            lbpimg = feature.local_binary_pattern(roi, 8, 1,method=type)\n",
    "            \n",
    "            #Obtiene histograma\n",
    "            n_bins = int(lbpimg.max()) + 1            \n",
    "            #feath, bins = np.histogram(lbpimg.ravel(), normed=True, bins=n_bins, range=(0,n_bins))\n",
    "            #feath, bins = np.histogram(lbpimg, normed=False, bins=n_bins, range=(0,n_bins)) #normed deprecated\n",
    "            feath, bins = np.histogram(lbpimg, density=False, bins=n_bins, range=(0,n_bins))\n",
    "            \n",
    "            # Concatena con celdas previas\n",
    "            LBPu_hist= np.concatenate([LBPu_hist , feath])\n",
    "            \n",
    "    return LBPu_hist\n",
    "\n",
    "def GetHOG(X, ncellsx, ncellsy, width, height):\n",
    "    Xhog = []\n",
    "    for i in range(len(X)):\n",
    "        #Recompone imagen\n",
    "        gray = X[i].reshape(height, width)\n",
    "        \n",
    "        # HOG            \n",
    "        feat_hog, hog_image = hog(gray, orientations=8, pixels_per_cell=(height/ncellsy, width/ncellsx),\n",
    "                    cells_per_block=(1, 1), visualize=True, block_norm='L1')   #multichannel=False,  elimindo en versiones recientes\n",
    "        Xhog.append(feat_hog)\n",
    "            \n",
    "    return Xhog\n",
    "        \n",
    "def GetLBP(X, ncellsx, ncellsy, width, height, lbptype):\n",
    "    Xlbp = []\n",
    "    for i in range(len(X)):\n",
    "        #Recompone imagen\n",
    "        gray = X[i].reshape(height, width)\n",
    "        \n",
    "        # LBP\n",
    "        feat_lbp = lbphist(gray, ncellsx, ncellsy, width, height, lbptype) \n",
    "        Xlbp.append(feat_lbp)\n",
    "                \n",
    "    return Xlbp\n",
    "\n",
    "def GetPredictions(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"SVM Normalization...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(X_train)\n",
    "    test_X = scaler.transform(X_test)\n",
    "\n",
    "    print(\"SVM training...\")\n",
    "    t0 = time()\n",
    "    parameters = {'C': [1e3, 5e3],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01],}\n",
    "    # Grid searach across parameter range\n",
    "    clf = GridSearchCV(\n",
    "        SVC(kernel='rbf', class_weight='balanced'), parameters, cv=5\n",
    "    )\n",
    "    clf = clf.fit(train_X, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "\n",
    "    print(\"Predicting\")\n",
    "    t0 = time()\n",
    "    # test labels\n",
    "    y_pred = clf.predict(test_X)\n",
    "   \n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    \n",
    "    return y_pred, y_test\n",
    "\n",
    "def GetPredictionsandProbs(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"SVM Normalization...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(X_train)\n",
    "    test_X = scaler.transform(X_test)\n",
    "\n",
    "    print(\"SVM training...\")\n",
    "    t0 = time()\n",
    "    parameters = {'C': [1e3, 5e3],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01],}\n",
    "    # Grid search across parameter range\n",
    "    clf = GridSearchCV(\n",
    "        SVC(kernel='rbf', class_weight='balanced', probability=True), parameters, cv=5\n",
    "    )\n",
    "    clf = clf.fit(train_X, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "\n",
    "    print(\"Predicting\")\n",
    "    t0 = time()\n",
    "    # test labels\n",
    "    y_pred = clf.predict(test_X)\n",
    "    \n",
    "    # train and test class probabilities\n",
    "    ytrain_prob = clf.predict_proba(train_X)\n",
    "    ytest_prob = clf.predict_proba(test_X)\n",
    "    \n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    \n",
    "    return y_pred, y_test, ytrain_prob, ytest_prob\n",
    "\n",
    "\n",
    "def LoadDataset(folder, ext):\n",
    "    # Contador de número de clases del conjunto\n",
    "    nclasses = 0\n",
    "    # Contador de muestras por clase\n",
    "    nperclass = []\n",
    "    # Etiqueta de cada clase (nombre de la subcarpeta)\n",
    "    classlabels = []\n",
    "    # Inicializa estructuras de datos y sus correpondientes etiquetas\n",
    "    X = []\n",
    "    Y = []\n",
    "   \n",
    "    # Asume que en la ruta indicada hay una subcarpeta por clase\n",
    "    for class_name in os.listdir(folder):\n",
    "        # Cada subcarpeta implica una clase más\n",
    "        nclasses += 1\n",
    "        # Inicialmente esta clase no tiene muestras\n",
    "        nsamples = 0    \n",
    "\n",
    "        # Compone la ruta\n",
    "        class_folder = os.path.join(folder, class_name)\n",
    "        for file_name in os.listdir(class_folder):\n",
    "            # Imágenes en formato ext\n",
    "            if file_name.endswith(ext):\n",
    "                # Lee la imagen\n",
    "                image = cv2.imread (os.path.join(class_folder, file_name))\n",
    "\n",
    "                # Lugar para aplicar un RoI sobre la imagen de entrada\n",
    "                # Ejemplo para la mitad superior\n",
    "                # image = image[0:int(height/2),:]\n",
    "\n",
    "                # Extrae tamaños\n",
    "                height, width, depth = image.shape\n",
    "                # Convierte  grises\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                # Añade a X, previa conversión en vector\n",
    "                X.append(gray.reshape(height*width))\n",
    "\n",
    "                # Añade etiqueta numérica de la muestra\n",
    "                Y.append(nclasses-1)\n",
    "\n",
    "                #Incrementa el número de muestras\n",
    "                nsamples += 1\n",
    "        \n",
    "        nperclass.append(nsamples)\n",
    "        classlabels.append(class_name)\n",
    "    \n",
    "    return X, Y, nsamples, class_name, nperclass, classlabels, width, height\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y presentación de datos\n",
    "\n",
    "Proporciona carpeta (ADAPTA A TU EQUIPO TRAS DECARGAR DATASET), carga datos mostrando primera muestra de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFICAR INDICANDO RUTA EN TU EQUIPO. EVITAR TILDES\n",
    "folder = \"C:/Users/otsed/Desktop/Docencia/VC/DatabaseGender59x65\" #portátil\n",
    "\n",
    "print('Loading dataset')\n",
    "X, Y, nsamples, class_name, nperclass, classlabels, width, height = LoadDataset(folder,'.jpg')\n",
    "\n",
    "#Convierte a numpy array X e Y\n",
    "X = np.array(X,dtype='float32')\n",
    "Y = np.array(Y,dtype='float64')\n",
    "\n",
    "# Obtiene número de muestras y características\n",
    "n_samples , n_features = X.shape\n",
    "# Obtiene nombres de las clases\n",
    "class_names = np.array(classlabels)\n",
    "n_classes = class_names.shape[0]\n",
    "\n",
    "print(\"Dataset info:\")\n",
    "print(\"# samples: %d\" % n_samples)\n",
    "print(\"# featues: %d\" % n_features)\n",
    "print(\"# classes: %d\" % n_classes)\n",
    "print(\"classes %s\" % classlabels)\n",
    "print(\"samples per class %s\" % str(nperclass)[1:-1] )\n",
    "        \n",
    "#Muestra primeras muestras de cada clase                                                   \n",
    "# Valor de resolución por defecto de matplotlib\n",
    "dpi = matplotlib.rcParams['figure.dpi']\n",
    "# Imágenes de muestra de cada clase\n",
    "nims2show = 3\n",
    " \n",
    "for nc in range(n_classes):\n",
    "    nimgs = 0\n",
    "    i = 0\n",
    "    while i < nims2show and nimgs < n_samples:\n",
    "        #Es muestra de la clase que interesa\n",
    "        if Y[nimgs] == nc:\n",
    "            gray = X[nimgs].reshape(height, width)\n",
    "            #Con la primera define configuración\n",
    "            if i == 0 and nc == 0:                    \n",
    "                figsize = 15*width / float(dpi), 15*height / float(dpi)\n",
    "                fig = plt.figure(figsize=figsize)                \n",
    "                        \n",
    "            fig.add_subplot(nc + 1,nims2show, i + 1)\n",
    "            plt.imshow(gray, cmap='gray', vmin=0, vmax=255) \n",
    "\n",
    "            i += 1 \n",
    "        nimgs += 1\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtiene HOG y LBP para determinada configuración de celdas\n",
    "#Pueden probarse otras configuraciones\n",
    "Xhog4x4 = GetHOG(X, 4, 4, width, height)\n",
    "Xlbp3x3 = GetLBP(X, 3, 3, width, height, \"nri_uniform\") #uniform a secas es también rotation invariant\n",
    "    \n",
    "#Convierte a numpy array X e Y\n",
    "Xhog4x4 = np.array(Xhog4x4,dtype='float32')\n",
    "Xlbp3x3 = np.array(Xlbp3x3,dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diseña conjunto experimental k-fold\n",
    "\n",
    "Divide los datos k veces en conjunto de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold\n",
    "# Define el número de subconjuntos a considerar\n",
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=4, shuffle=True)\n",
    "#Distribución de muestras por fold\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"Fold %d\" % fold)\n",
    "    print(\"# samples in training set %d\" % train_index.shape[0])\n",
    "    print(\"# samples in test set %d\" % test_index.shape[0])\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para cada fold\n",
    "Cálculo en primer término de PCA (95% de la varianza) de las muestras de entrenamiento. Posteriormente:\n",
    "- Clasifica con píxeles como características y KNN\n",
    "- Clasifica con componentes PCA como características con KNN\n",
    "- Clasifica con componentes PCA como características con SVM\n",
    "- Clasifica con LBP como características con SVM\n",
    "- Clasifica con HOG como características con SVM\n",
    "- Stacking de los tres últimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA on the training subset for n_comp components\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "n_comp = 150\n",
    "\n",
    "# KNN parámetros\n",
    "knn = 5\n",
    "\n",
    "#Valores de precision y recall\n",
    "precs_px_knn, recs_px_knn = [], []\n",
    "precs_pca_knn, recs_pca_knn = [], []\n",
    "precs_pca_svm, recs_pca_svm = [], []\n",
    "precs_hog_svm, recs_hog_svm = [], []\n",
    "precs_lbp_svm, recs_lbp_svm = [], []\n",
    "precs_stk_svm, recs_stk_svm = [], []\n",
    "\n",
    "# Recorre folds\n",
    "fold = 1\n",
    "while fold <= kfold:\n",
    "    accs, precs, recs = [], [], []\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        print(\"***\\nFold %d\" % fold)\n",
    "        #División de muestras de entreno y test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        #Etiquetas de las muestras\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #Muestras HOG y LBP de entreno y test\n",
    "        Xhog4x4_train, Xhog4x4_test = Xhog4x4[train_index], Xhog4x4[test_index]\n",
    "        Xlbp3x3_train, Xlbp3x3_test = Xlbp3x3[train_index], Xlbp3x3[test_index]\n",
    "        \n",
    "        #####################################\n",
    "        # Píxeles y KNN\n",
    "        print(\"\\nPXL+KNN\")\n",
    "        t0 = time()\n",
    "        # k = 5 \n",
    "        model_px = KNeighborsClassifier(n_neighbors = knn) \n",
    "\n",
    "        # fdtraining of model \n",
    "        model_px.fit(X_train, y_train) \n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Predicting...\")\n",
    "        t0 = time()\n",
    "        y_pred=model_px.predict(X_test)\n",
    "\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Classification results:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))       \n",
    "        \n",
    "        precs_px_knn.append(precision_score(y_test, y_pred))\n",
    "        recs_px_knn.append(recall_score(y_test, y_pred))\n",
    "    \n",
    "        # Confusion matriz just for first fold\n",
    "        if fold == 1:\n",
    "            print(\"Confussion matrix:\")\n",
    "            print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "        \n",
    "        ##############################################\n",
    "        print(\"PCA computation for .95 of the variance from %d faces...\"\n",
    "        % (X_train.shape[0]))\n",
    "        t0 = time()\n",
    "        pca_95 = PCA(.95).fit(X_train)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"%d components selected\" % (pca_95.n_components_ ))\n",
    "\n",
    "        eigenfaces_95 = pca_95.components_.reshape((pca_95.n_components_ , height, width))\n",
    "\n",
    "        print(\"Projecting training and test on the eigenfaces orthonormal basis\")\n",
    "        t0 = time()\n",
    "        X_train_pca_95 = pca_95.transform(X_train)\n",
    "        X_test_pca_95 = pca_95.transform(X_test)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #####################################\n",
    "        # PCA y KNN\n",
    "        print(\"\\nPCA+KNN\")\n",
    "        print(\"KNN (k=%d) classifier based on %d PCA components...\" % (knn, pca_95.n_components_) ) \n",
    "        t0 = time()\n",
    "        model_pca_95 = KNeighborsClassifier(n_neighbors = knn) \n",
    "\n",
    "        # fdtraining of model \n",
    "        model_pca_95.fit(X_train_pca_95, y_train) \n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Predicting...\")\n",
    "        t0 = time()\n",
    "        y_pred=model_pca_95.predict(X_test_pca_95)\n",
    "\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"\\nMetrics\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        precs_pca_knn.append(precision_score(y_test, y_pred))\n",
    "        recs_pca_knn.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "        if fold == 1:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "        \n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "        #####################################\n",
    "        # PCA y SVM\n",
    "        print(\"\\nPCA+SVM\")        \n",
    "        y_pred, y_test = GetPredictions(X_train_pca_95, X_test_pca_95,y_train, y_test)\n",
    "        print(\"\\nMetrics\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        precs_pca_svm.append(precision_score(y_test, y_pred))\n",
    "        recs_pca_svm.append(recall_score(y_test, y_pred))\n",
    "\n",
    "        if fold == 1:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "            \n",
    "        #####################################\n",
    "        # HOG y SVM\n",
    "        print(\"\\nHOG+SVM\")\n",
    "        y_pred, y_test = GetPredictions(Xhog4x4_train, Xhog4x4_test,y_train, y_test)\n",
    "        print(\"\\nHOG Metrics\")\n",
    "        precs_hog_svm.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recs_hog_svm.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        if fold == 1:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))        \n",
    "        \n",
    "        #####################################\n",
    "        # LBP y SVM\n",
    "        print(\"\\nLBP+SVM\")\n",
    "        y_pred, y_test = GetPredictions(Xlbp3x3_train, Xlbp3x3_test,y_train, y_test)\n",
    "        print(\"\\nLBP Metrics\")\n",
    "        precs_lbp_svm.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recs_lbp_svm.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        if fold == 1:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        #Ensemble\n",
    "        print(\"\\nEnsemble/Stacking\")\n",
    "        #Obtiene también probs\n",
    "        y_pred, y_test, pca95_train_prob, pca95_test_prob = GetPredictionsandProbs(X_train_pca_95, X_test_pca_95,y_train, y_test)\n",
    "        y_pred, y_test, hog_train_prob, hog_test_prob = GetPredictionsandProbs(Xhog4x4_train, Xhog4x4_test,y_train, y_test)\n",
    "        y_pred, y_test, lbp_train_prob, lbp_test_prob = GetPredictionsandProbs(Xlbp3x3_train, Xlbp3x3_test,y_train, y_test)\n",
    "        #Score level fusion, using reported probabilities. Tranposr matrix is later used\n",
    "        X_train = np.vstack((pca95_train_prob[:,1],hog_train_prob[:,1],lbp_train_prob[:,1]))\n",
    "        X_test = np.vstack((pca95_test_prob[:,1],hog_test_prob[:,1],lbp_test_prob[:,1]))\n",
    "        y_pred, y_test = GetPredictions(X_train.T,X_test.T,y_train, y_test)\n",
    "            \n",
    "        print(\"\\nEnsemble/Stacking Metrics\")\n",
    "        precs_stk_svm.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recs_stk_svm.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "        \n",
    "        fold += 1\n",
    "\n",
    "print(\"\\n********K-FOLD SUMMARY*********\")\n",
    "print(\"PX+KNN Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_px_knn) , np.mean(recs_px_knn) )) \n",
    "print(\"PCA+KNN Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_pca_knn) , np.mean(recs_pca_knn) )) \n",
    "print(\"PCA+SVM Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_pca_svm) , np.mean(recs_pca_svm) )) \n",
    "print(\"HOG+SVM Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_hog_svm) , np.mean(recs_hog_svm) )) \n",
    "print(\"LBP+SVM Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_lbp_svm) , np.mean(recs_lbp_svm) )) \n",
    "print(\"Ensemble Mean Precision:  %0.3f, Mean Recall:  %0.3f\" % ( np.mean(precs_stk_svm) , np.mean(recs_stk_svm) )) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('FACES')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea3a1ee99ce326e593ddb52cd278556d527fcb6552c40e2a47b1efb9d0183637"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
